# ğŸ”¬ OpenSci Skill

<p align="center">
  <strong>A Meta-Skill for Building Agent Knowledge Bases for Scientific Python Libraries</strong>
</p>

<p align="center">
  <a href="./README.md">ä¸­æ–‡</a> Â·
  <a href="#quick-start">Quick Start</a> Â·
  <a href="#how-it-works">How It Works</a> Â·
  <a href="#contributing-skills">Contributing Skills</a>
</p>

---

## ğŸ§© What Is This

**OpenSci Skill** is a meta-skill â€” it doesn't target any single scientific library. Instead, it teaches an AI Agent **how to generate a high-quality, reusable Skill knowledge file for any scientific Python library**.

Using OpenSci Skill, an Agent can automatically:
- ğŸ•·ï¸ Crawl and parse a library's official documentation
- ğŸ” Extract the module structure and public API signatures
- ğŸ“– Build a Symbol Index for precise function lookup
- ğŸ—‚ï¸ Organize knowledge into structured reference files by functional domain
- ğŸ“„ Output a standardized `SKILL.md` navigator entrypoint

The resulting Skill can be loaded directly by AI Agents that support the Skill format (e.g., GitHub Copilot), enabling **accurate, up-to-date, and reusable** library usage capabilities for that specific domain.

---

## ğŸ’¡ Motivation: Why This Project Exists

### âŒ Problem 1 â€” Inaccurate Tool Usage in Specialized Domains

Large language models have strong coverage of general-purpose code, but their knowledge of **niche scientific libraries** (MNE, nilearn, PyMC, scanpy, etc.) is often frozen at training time. Common failure modes include:

- Calling deprecated APIs
- Confusing parameter signatures across versions
- Generating incorrect usage for library-specific data containers (e.g., `Raw`, `AnnData`)
- Mishandling optional dependency imports and install paths

### âŒ Problem 2 â€” Knowledge Becomes Stale

LLMs have a knowledge cutoff. Scientific libraries release frequently, and APIs change. Without an external knowledge injection mechanism, an Agent has no reliable way to answer "what is the current signature of this function?".

### âŒ Problem 3 â€” Knowledge Is Not Reusable

Each time an Agent is configured for a new scientific library use case, someone must re-upload docs, re-engineer prompts, and re-validate code examples. This work **cannot be reused across projects, teams, or Agents**.

### âŒ Problem 4 â€” Human-Readable Docs Are a Context Burden for AI

Guidebooks, tutorials, and API references written for human readers are filled with narrative prose, repeated background explanations, and redundant examples. For an AI Agent, this content **directly consumes precious context window space** while delivering almost no incremental information value â€” the LLM already knows the general concepts. What actually needs to be injected is what the model *doesn't know*: exact versioned signatures, library-specific container semantics, and migration paths for deprecated APIs.

OpenSci Skill is designed with the Agent as the primary reader. It **aggressively strips redundancy** and retains only high-density knowledge that cannot be reliably derived from the model's training data.

### âœ… OpenSci Skill's Solution

| Problem | Solution |
|---------|----------|
| âŒ Inaccurate tool calls | Extract API from the actual installed package / source; bind signatures to a specific version |
| âŒ Stale knowledge | Skills can be regenerated per library version; `version.txt` records the exact build version |
| âŒ Non-reusable knowledge | Standardized Skill directory layout, directly loadable by any compatible Agent |
| âŒ Docs are a context burden | Agent-first output format â€” strips narrative; keeps only high-density library-specific knowledge |

---

## âš™ï¸ How It Works

### ğŸ“ Skill Directory Layout

Every library skill produced by OpenSci Skill follows a consistent structure:

```
<library-name>/
â”œâ”€â”€ SKILL.md                  # Navigator entrypoint (required)
â”œâ”€â”€ assets/                   # Auto-generated, machine-readable artifacts
â”‚   â”œâ”€â”€ version.txt           # Library version, Python version, build date
â”‚   â”œâ”€â”€ module-map.md         # Module structure map
â”‚   â”œâ”€â”€ symbol-index.md       # Symbol dictionary index (human-readable)
â”‚   â”œâ”€â”€ symbol-index.jsonl    # Symbol dictionary index (machine-readable)
â”‚   â”œâ”€â”€ symbol-cards/         # Per-module symbol cards
â”‚   â””â”€â”€ docs-cache/           # Crawled/parsed official documentation cache
â”œâ”€â”€ references/
â”‚   â”œâ”€â”€ <domain-1>.md         # Deep reference per functional domain
â”‚   â””â”€â”€ ...
â””â”€â”€ scripts/                  # Optional: helper scripts
```

### ğŸšï¸ Three Depth Modes

| Mode | Source | Speed | When to Use |
|------|--------|-------|-------------|
| **Light** | Crawl official online docs only | Fast | Well-documented public libraries |
| **Medium** | Docs + verified code examples | Moderate | Need confirmed API behavior |
| **Heavy** | Full source traversal + paper links | Slow | Sparse-doc research libraries |

### ğŸ“Š Three Coverage Profiles

| Profile | Goal | Best For |
|---------|------|----------|
| **Workflow** | High-quality guidance for common tasks | Task-focused assistants |
| **Dictionary** | Broad API symbol lookup | Knowledge-base style assistants |
| **Hybrid** | Workflow references + dictionary assets (default) | General-purpose Agents |

### ğŸ› ï¸ Core Helper Scripts

| Script | Purpose |
|--------|---------|
| `scripts/map-modules.py` | Extract module structure and `__init__.py` exports |
| `scripts/fetch-docs.py` | Crawl official online docs â†’ `docs-cache/` |
| `scripts/fetch-local-rst.py` | Parse local Sphinx RST documentation |
| `scripts/extract-api-patterns.py` | Extract public API signatures â†’ `api-dump.md` |
| `scripts/build-symbol-index.py` | Build symbol index and symbol cards |
| `scripts/verify-snippets.py` | Execute and validate all code snippets |

### ğŸ”„ End-to-End Pipeline

```
Select depth mode and coverage profile
        â†“
Record environment & install permissions (version.txt)
        â†“
Build module map (map-modules.py)
        â†“
Gather documentation (fetch-docs.py / fetch-local-rst.py)
        â†“
Build symbol index (build-symbol-index.py)            â† Dictionary/Hybrid
        â†“
Extract API patterns (extract-api-patterns.py)         â† Medium/Heavy
        â†“
Split functional domains, write references/<domain>.md
        â†“
Write target library SKILL.md
        â†“
Quality gate (authoring-checklist.md + verify-snippets.py)
```

---

## ğŸš€ Quick Start

### Generate a Skill for a Library

Mount the `opensci-skill/` directory (this repository) into your Agent's accessible workspace, then trigger the Agent:

```
Create an opensci skill for <library-name>
```

The Agent will run the full pipeline and output a standardized Skill under `<library-name>/`.

### Trigger Keywords (for Agents with Skill support)

```
write skill | create skill | new skill | opensci skill |
skill for library | audit skill | library skill | api dictionary
```

### Reference Documents

- [references/skill-template.md](references/skill-template.md) â€” Template skeleton for a target library's `SKILL.md`
- [references/reference-file-template.md](references/reference-file-template.md) â€” Template skeleton for `references/<domain>.md`
- [references/authoring-checklist.md](references/authoring-checklist.md) â€” Pre-delivery quality gate checklist

---

## ğŸ¤ Contributing Skills

> **ğŸ¯ Goal: Build a public Agent knowledge base for the open-source community**

Every library Skill generated using OpenSci Skill is a reusable knowledge asset for the community. We invite you to:

1. Use OpenSci Skill to generate Skills for scientific libraries you know well â€” NumPy, SciPy, Pandas, scikit-learn, MNE, nilearn, PyMC, scanpy, xarray, zarr, and many more
2. Submit the generated `<library-name>/` directory via Pull Request
3. Propose additional libraries you'd like covered in Issues

**The effort to contribute a Skill varies by library size and documentation quality. In Light mode, it typically takes only a few minutes.**

### Contribution Rules

- **Forbidden files** inside any Skill folder: `README.md`, `CHANGELOG.md`, `CONTRIBUTING.md`
- `assets/version.txt` must be present and explicitly state the build version
- All code examples must be runnable, or have an explicit data-dependency note
- Run `scripts/verify-snippets.py` to zero errors before submitting

### Libraries We'd Love Covered

Some high-priority candidates for community Skills:

| Domain | Libraries |
|--------|-----------|
| Neuroscience | MNE-Python, nilearn, Neo, Brian2 |
| Biology / Genomics | scanpy, anndata, biopython, pysam |
| Statistics / Probabilistic ML | PyMC, numpyro, arviz, bambi |
| Earth / Climate | xarray, cf-xarray, MetPy, cartopy |
| Image / Signal | scikit-image, pywavelets, torchaudio |
| Materials / Chemistry | pymatgen, ASE, RDKit, mendeleev |

---

## ğŸ§­ Design Principles

- ğŸ¤– **Agent-first, not human-first**: All output format, density, and organization is optimized for LLM context consumption
- ğŸ“Œ **Version-bound**: Every Skill is bound to the exact version it was built against, eliminating version-drift hallucinations
- ğŸ’° **Token economy**: Aggressively drop content LLMs already know; keep only library-specific high-value knowledge
- âœ… **Verifiability**: All code examples must be executable; unverified claims are explicitly tagged `[UNVERIFIED]`
- ğŸšï¸ **Progressive depth**: Three-tier Light/Medium/Heavy modes â€” invest resources proportional to the library's complexity

---

## ğŸ“œ License

MIT

---

<p align="center">
If this project helps you, please consider giving it a â­ and sharing it with others working on AI-assisted scientific computing.
</p>
